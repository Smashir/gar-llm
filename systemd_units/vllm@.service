[Unit]
Description=vLLM API Server for %i
After=network.target

[Service]
Type=simple
User=aiuser
WorkingDirectory=/home/aiuser
EnvironmentFile=/home/aiuser/systemd_units/env/%i.env
Restart=always
RestartSec=5

# Bash の -lc を使わず、変数展開と実行を安全に行う
ExecStart=/bin/bash -c '\
ARGS="--model ${MODEL_PATH:-} \
--host ${HOST:-0.0.0.0} --port ${PORT:-8000} \
--dtype ${DTYPE:-auto} --max-model-len ${MAX_MODEL_LEN:-8192} \
--gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.90} \
--max-num-seqs ${MAX_NUM_SEQS:-8} --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}"; \
if [[ -n "${QUANTIZATION:-}" && "${QUANTIZATION:-None}" != "None" ]]; then \
  ARGS="$ARGS --quantization ${QUANTIZATION}"; \
fi; \
if [[ -n "${ATTN_BACKEND:-}" ]]; then \
  ARGS="$ARGS --attn-backend ${ATTN_BACKEND}"; \
fi; \
if [[ -n "${KV_CACHE_DTYPE:-}" ]]; then \
  ARGS="$ARGS --kv-cache-dtype ${KV_CACHE_DTYPE}"; \
fi; \
echo "[vLLM start] /home/aiuser/ai_envs/vllm_env/bin/python -m vllm.entrypoints.openai.api_server $ARGS"; \
exec /home/aiuser/ai_envs/vllm_env/bin/python -m vllm.entrypoints.openai.api_server $ARGS \
'

[Install]
WantedBy=multi-user.target




